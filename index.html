
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="hustcat" />
    <title>GPU计算 -- GPU体系结构及CUDA编程模型</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="hustcat" type="application/atom+xml" />
    <link rel="stylesheet" href="/media/css/style.css" />
    <link rel="stylesheet" href="/media/css/highlight.css" />
    <script type="text/javascript" src="/media/js/jquery-1.7.1.min.js"></script>
  </head>
  <body>
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>GPU计算 -- GPU体系结构及CUDA编程模型</h1>
        </header>
        <nav>
        <span><a title="网站首页" class="" href="/">首页</a></span>
        <span><a title="文章分类" class="" href="/categories/">分类</a></span>
        <span><a title="标签索引" class="" href="/tags/">标签</a></span>
        <!--<span><a title="友情链接" class="" href="/links/">链接</a></span>-->
        <span><a title="留言交流" class="" href="/guestbook/">留言</a></span>
        <span><a title="关于站长" class="" href="/about/">关于</a></span>
        <span><a title="种子订阅" class="" href="/feed/" target="_blank">订阅</a></span>
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2017-11-11">2017-11-11</time>
</span>

 | 
<span class="categories">
  分类
  
  <a href="/categories/#GPU" title="GPU">GPU</a>&nbsp;
  
</span>


 | 
<span class="tags">
  标签
  
  <a href="/tags/#GPU" title="GPU">GPU</a>&nbsp;
  
  <a href="/tags/#CUDA" title="CUDA">CUDA</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="体系结构">体系结构</h2>

<h3 id="cpu与gpu">CPU与GPU</h3>

<p>GPU与CPU设计处理的计算任务的目标是不一样的，导到两者的整体结构有很大的区别：</p>

<p><img src="/assets/GPU/cpu_vs_gpu_00.png" alt="" /></p>

<p>具体来说，CPU是一种低延迟的设计:</p>

<p><img src="/assets/GPU/cpu_feature_00.png" alt="" /></p>

<p>(1) CPU有强大的ALU，时钟频率很高；
(2) CPU的容量较大的cache，一般包括L1､L2和L3三级高速缓存；L3可以达到8MB，这些cache占据相当一部分片上空间；
(3) CPU有复杂的控制逻辑，例如：复杂的流水线（pipeline)、分支预测（branch prediction）、乱序执行（Out-of-order execution）等；</p>

<p>这些设计使得真正进行计算的ALU单元只占据很小一部分片上空间。</p>

<p> 而GPU是一种高吞吐的设计，具体来说：</p>

<p><img src="/assets/GPU/gpu_feature_00.png" alt="" /></p>

<p>(1) GPU有大量的ALU；
(2) cache很小；缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为thread提高服务的；
(2) 没有复杂的控制逻辑，没有分支预测等这些组件；</p>

<p>总的来说，CPU擅长处理逻辑复杂、串行的计算任务；而GPU擅长的是大规模的数据并行（data-parallel）的计算任务。</p>

<h3 id="gpu体系结构">GPU体系结构</h3>

<ul>
  <li>GPU内部结构</li>
</ul>

<p>一个“假想”的<code class="language-plaintext highlighter-rouge">GPU Core</code>结构如下：</p>

<p><img src="/assets/GPU/gpu_core_inside.png" alt="" /></p>

<p>它包括8个ALU，4组执行环境（Execution context），每组有8个Ctx。这样，一个Core可以并发(concurrent but interleaved)执行4条指令流（instruction streams），32个并发程序片元(fragment)。</p>

<p>我们用16个上面的Core构成一个GPU，如下：</p>

<p><img src="/assets/GPU/gpu_inside.png" alt="" /></p>

<p>这样，一个GPU有16个Core、128个ALU，可以同时处理16条指令流、64条并发指令流、512(32*16)个并发程序片元。</p>

<ul>
  <li>示例</li>
</ul>

<p>以<code class="language-plaintext highlighter-rouge">NVIDIA GeForce GTX 580</code>为例，每个<code class="language-plaintext highlighter-rouge">GPU Core</code>的内部如下：</p>

<p><img src="/assets/GPU/gtx_580_core_inside.png" alt="" /></p>

<p>每个Core有64个<code class="language-plaintext highlighter-rouge">CUDA core（也叫做Stream Processor, SP）</code>，每个<code class="language-plaintext highlighter-rouge">CUDA core</code>可以理解为一个复杂完整的ALU。这些<code class="language-plaintext highlighter-rouge">CUDA core</code>，分成2组，每组32个<code class="language-plaintext highlighter-rouge">CUDA core</code>，共享相同的取指／译码部件，这一组称为<code class="language-plaintext highlighter-rouge">Stream Multiprocessor（SM）</code> 。</p>

<p>每个Core可以<code class="language-plaintext highlighter-rouge">并发</code>执行1536个程序片元，即1536个<code class="language-plaintext highlighter-rouge">CUDA threads</code>。</p>

<p>一个<code class="language-plaintext highlighter-rouge">GTX 580</code>GPU包含16个Core，总共1024个<code class="language-plaintext highlighter-rouge">CUDA core</code>，可以<code class="language-plaintext highlighter-rouge">并发</code>执行24576(1536*16)个<code class="language-plaintext highlighter-rouge">CUDA threads</code>.</p>

<p><img src="/assets/GPU/gtx_580_core.png" alt="" /></p>

<h3 id="数据存储">数据存储</h3>

<p>CPU的典型存储结构如下：</p>

<p><img src="/assets/GPU/cpu_cache.png" alt="" /></p>

<p>一般来说，CPU和内存之间的带宽只有数十GB/s。比如对于<a href="https://ark.intel.com/zh-cn/products/81061/Intel-Xeon-Processor-E5-2699-v3-45M-Cache-2_30-GHz">Intel Xeon E5-2699 v3</a>，内存带宽达到68GB/s（(2133 * 64 / 8)*4 MB/s）:</p>

<table>
  <thead>
    <tr>
      <th>内存规格</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>最大内存大小（取决于内存类型）</td>
      <td>768 GB</td>
    </tr>
    <tr>
      <td>内存类型</td>
      <td>DDR4 1600/1866/2133</td>
    </tr>
    <tr>
      <td>最大内存通道数</td>
      <td>4</td>
    </tr>
    <tr>
      <td>最大内存带宽</td>
      <td>68 GB/s</td>
    </tr>
  </tbody>
</table>

<p>而GPU的存储结构一般如下:</p>

<p><img src="/assets/GPU/gpu_cache.png" alt="" /></p>

<p>GPU的高速缓存较小，上图的Memory实际上是指GPU卡内部的显存。但是与显存之间的带宽可以达到数百GB/s，比如P40的显存带宽为346GB/s，远远大于CPU的内存带宽，但是，相对于GPU的计算能力，显存仍然是瓶颈所在。</p>

<h3 id="cpu与gpu交互">CPU与GPU交互</h3>

<p>在现代的异构计算系统中，GPU是以PCIe卡作为CPU的外部设备存在，两者之间通过PCIe总线通信:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ----------           ------------
|___DRAM___|         |___GDRAM____|
      |                    |
 ----------           ------------
|   CPU    |         |    GPU     |
|__________|         |____________|
      |                    |
  ---------            --------
 |___IO____|---PCIe---|___IO___|

</code></pre></div></div>

<p>对于<code class="language-plaintext highlighter-rouge">PCIe Gen3 x1</code>理论带宽约为1000MB/s，所以对于<code class="language-plaintext highlighter-rouge">Gen3 x32</code>的最大带宽为~32GB/s，而受限于本身的实现机制，有效带宽往往只有理论值的2/3还低。所以，CPU与GPU之间的通信开销是比较大的。</p>

<h2 id="cuda编程模型">CUDA编程模型</h2>

<p>在对GPU的体系结构有了基本的了解之后，来看看CUDA的编程模型，这是进行CUDA编程的基础。</p>

<h3 id="kernel">Kernel</h3>

<p>一个CUDA程序的可以分为两个部分: 在CPU上运行的<code class="language-plaintext highlighter-rouge">Host</code>程序；在GPU上运行的<code class="language-plaintext highlighter-rouge">Device</code>程序。两者拥有各自的存储器。GPU上运行的函数又被叫做<code class="language-plaintext highlighter-rouge">kernel</code>函数，通过<code class="language-plaintext highlighter-rouge">__global__</code>关键字声名，例如：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Kernel definition
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}
int main()
{
...
    // Kernel invocation with N threads
    VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);
... 
}
</code></pre></div></div>

<p>Host程序在调用Device程序时，可以通过<code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;...&gt;&gt;&gt;</code>中的参数提定执行该<code class="language-plaintext highlighter-rouge">kernel</code>的<code class="language-plaintext highlighter-rouge">CUDA threads</code>的数量。每个Thread在执行Kernel函数时，会被分配一个<code class="language-plaintext highlighter-rouge">thread ID</code>，kernel函数可以通过内置变量<code class="language-plaintext highlighter-rouge">threadIdx</code>访问。</p>

<h3 id="线程层次-thread-hierarchy">线程层次 (Thread Hierarchy)</h3>

<p>CUDA中的线程组织为三个层次<code class="language-plaintext highlighter-rouge">Grid</code>、<code class="language-plaintext highlighter-rouge">Block</code>、<code class="language-plaintext highlighter-rouge">Thread</code>。<code class="language-plaintext highlighter-rouge">threadIdx</code>是一个<code class="language-plaintext highlighter-rouge">3-component</code>向量(vector)，所以线程可以使用1维、2维、3维的线程索引(thread index)来标识。同时由多个线程组成的<code class="language-plaintext highlighter-rouge">thread block</code>也可以分别是1维、2维或者3维的。</p>

<blockquote>
  <p>For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block.</p>
</blockquote>

<p><img src="/assets/GPU/thread_hierarchy.png" alt="" /></p>

<p>每个块(Block)所能包含的线程(Thread)数量是有限制的，因为目前每个块内的所有线程都是在一个物理的处理器核中，并且共享了这个核有限的内存资源。当前的GPU中，每个块最多能执行1024个线程。</p>

<blockquote>
  <p>There is a limit to the number of threads per block, since all threads of a block are expected to reside on the same processor core and must share the limited memory resources of that core. On current GPUs, a thread block may contain up to 1024 threads</p>
</blockquote>

<p>多个<code class="language-plaintext highlighter-rouge">Blocks</code>可以组成1维、2维或者3维的<code class="language-plaintext highlighter-rouge">Grid</code>。kernel函数可以访问grid内部标识block的内置变量<code class="language-plaintext highlighter-rouge">blockIdx</code>，也可以访问表示block维度的内置变量<code class="language-plaintext highlighter-rouge">blockDim</code>.</p>

<blockquote>
  <p>Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks.The number of thread blocks in a grid is usually dictated by the size of the data being processed or the number of processors in the system, which it can greatly exceed.</p>
</blockquote>

<ul>
  <li>thread index的计算</li>
</ul>

<p>根据thread、block、grid的维度不同，<code class="language-plaintext highlighter-rouge">thread index</code>的计算方式也不一样。这时考虑两种简单的情况：</p>

<p>(1) grid划分成1维，block划分为1维</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    int threadId = blockIdx.x *blockDim.x + threadIdx.x;  
</code></pre></div></div>

<p>(2) grid划分成1维，block划分为2维</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    int threadId = blockIdx.x * blockDim.x * blockDim.y+ threadIdx.y * blockDim.x + threadIdx.x;  
</code></pre></div></div>

<p>注意，如果将<code class="language-plaintext highlighter-rouge">threadIdx</code>看做一个2维矩阵的话，<code class="language-plaintext highlighter-rouge">threadIdx.y</code>确定行号，而<code class="language-plaintext highlighter-rouge">threadIdx.x</code>确定列号。</p>

<p>在调用kernel函数可以通过<code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;...&gt;&gt;&gt;</code>指定每个block的threads的数量，以及每个grid的blocks数量，例如：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i &lt; N &amp;&amp; j &lt; N)
        C[i][j] = A[i][j] + B[i][j];
}
int main()
{
...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);
    ...
}
</code></pre></div></div>

<p>GPU中的这种多维的线程结构，可以让线程非常方便的索引它需要处理的向量、矩阵、或者立方体数据结构的元素。</p>

<h3 id="gpu线程的映射">GPU线程的映射</h3>

<p>CUDA thread最终由实际的物理硬件计算单元执行，这里看看thread是如何映射到硬件单元的。先重复一下几个概念。</p>

<ul>
  <li>基本概念</li>
</ul>

<table>
  <thead>
    <tr>
      <th>简称</th>
      <th>全称</th>
      <th>注释</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SM</td>
      <td>Stream Multiprocessor</td>
      <td>实际上对应一个<code class="language-plaintext highlighter-rouge">CUDA core</code></td>
    </tr>
    <tr>
      <td>SP</td>
      <td>Stream Processor</td>
      <td>每个SM包含若干个SP, 由SM取指, 解码, 发射到各个SP, GPU可看作是一组SM</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>映射关系</li>
</ul>

<p>CUDA线程与硬件的具体的映射关系如下：</p>

<p>Thread -&gt; SP
Block  -&gt; SM
Grid -&gt; GPU</p>

<p>值得注意的是虽然Block映射到SM，但两者并不需要一一对应的关系。Blocks可以由任意数量的SM以任意的顺序调度，然后彼此独立的并行或者串行执行。这样，使得硬件的SM能够适应任意数量的<code class="language-plaintext highlighter-rouge">CUDA block</code>。</p>

<h3 id="内存层次-thread-hierarchy">内存层次 (Thread Hierarchy)</h3>

<p><code class="language-plaintext highlighter-rouge">CUDA threads</code>在执行时，可以访问多个<code class="language-plaintext highlighter-rouge">memory spaces</code>，每个线程有自己的私有的<code class="language-plaintext highlighter-rouge">local memory</code>。每个block有一个<code class="language-plaintext highlighter-rouge">shared memory</code>，block的所有线程都可以访问。最后，所有线程都可以访问<code class="language-plaintext highlighter-rouge">global memory</code>。</p>

<p><img src="/assets/GPU/memory_hierarchy.png" alt="" /></p>

<p>不同的内存访问速度如下：</p>

<blockquote>
  <p>本地内存 &gt; 共享内存 &gt; 全局内存</p>
</blockquote>

<p>通过<code class="language-plaintext highlighter-rouge">cudaMalloc</code>分配的内存就是全局内存。核函数中用<code class="language-plaintext highlighter-rouge">__shared__</code>修饰的变量就是共享内存。 核函数定义的变量使用的就是本地内存。</p>

<h2 id="refs">Refs</h2>

<ul>
  <li><a href="http://haifux.org/lectures/267/Introduction-to-GPUs.pdf">Introduction to GPU Architecture</a></li>
  <li><a href="http://www.cnblogs.com/biglucky/p/4223565.html"> 1.2CPU和GPU的设计区别</a></li>
  <li><a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C Programming Guide</a></li>
  <li><a href="https://andyhuzhill.github.io/parallel%20programming/2016/09/18/cuda-programming-model">CUDA 编程模型</a></li>
</ul>

</section>
<section align="right">
<br/>
<span>
	<a  href="/roce-protocol/" class="pageNav"  >上一篇</a>
	&nbsp;&nbsp;&nbsp;
	<a  href="/ovn-k8s-practice/" class="pageNav"  >下一篇</a>
</span>
</section>
<!-- JiaThis Button BEGIN -->
<script type="text/javascript">
var jiathis_config = {data_track_clickback:'true'};
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_r.js?move=0&amp;uid=2121774" charset="utf-8"></script>
<!-- JiaThis Button END -->


	
	<div class="ds-thread" />
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"hustcat"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = 'http://static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fc91691cf4004b194f7847896cca17dbb' type='text/javascript'%3E%3C/script%3E"));
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-56673760-1', 'auto');
  ga('send', 'pageview');

</script>

        </article>
      </div>

    <footer>
        <p>
          <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fc91691cf4004b194f7847896cca17dbb' type='text/javascript'%3E%3C/script%3E"));
</script>

          <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-56673760-1', 'auto');
  ga('send', 'pageview');

</script>
        </p>
        <p><small>
            <a href="http://github.com/hustcat/hustcat.github.io/new/gh-pages/_posts" target="_blank" title="撰写文章">Po</a>wer<a href="http://github.com/hustcat/hustcat.github.io/edit/gh-pages/_posts/2017-11-11-gpu-architecture.md" target="_blank" title="编辑页面">ed</a> by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> @ <a href="http://github.com/hustcat/hustcat.github.io" target="_blank" title="项目主页">GitHub</a>
             | <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/cn/" target="_blank" title="许可协议">©</a> 2014 - 2020 <a href="/about/">hustcat</a>
             | <a href="http://hustcat.cnblogs.com" target="_blank">@cnblogs</a>


         </small></p>
    </footer>

    </div>
  </body>
</html>
